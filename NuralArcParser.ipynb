{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbuFZ6OXryXs",
        "outputId": "4fad7f7e-3bfd-43da-9732-5e0e52c6fd51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.3)\n"
          ]
        }
      ],
      "source": [
        "pip install conllu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_72irLaTqK8S",
        "outputId": "40961340-1287-4e1a-c965-924c15f825c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[({'stack_top_id': 0, 'buffer_first_id': 1, 'stack_top_word': '.', 'buffer_first_word': 'al', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'PROPN'}, 'SHIFT'), ({'stack_top_id': 1, 'buffer_first_id': 2, 'stack_top_word': 'al', 'buffer_first_word': '-', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT'), ({'stack_top_id': 2, 'buffer_first_id': 3, 'stack_top_word': '-', 'buffer_first_word': 'zaman', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'PROPN'}, 'LEFT-ARC'), ({'stack_top_id': 1, 'buffer_first_id': 3, 'stack_top_word': 'al', 'buffer_first_word': 'zaman', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PROPN'}, 'RIGHT-ARC'), ({'stack_top_id': 1, 'buffer_first_id': 4, 'stack_top_word': 'al', 'buffer_first_word': ':', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT'), ({'stack_top_id': 4, 'buffer_first_id': 5, 'stack_top_word': ':', 'buffer_first_word': 'american', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'ADJ'}, 'SHIFT'), ({'stack_top_id': 5, 'buffer_first_id': 6, 'stack_top_word': 'american', 'buffer_first_word': 'forces', 'stack_top_pos': 'ADJ', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 4, 'buffer_first_id': 6, 'stack_top_word': ':', 'buffer_first_word': 'forces', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'NOUN'}, 'SHIFT'), ({'stack_top_id': 6, 'buffer_first_id': 7, 'stack_top_word': 'forces', 'buffer_first_word': 'killed', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'VERB'}, 'LEFT-ARC'), ({'stack_top_id': 4, 'buffer_first_id': 7, 'stack_top_word': ':', 'buffer_first_word': 'killed', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'VERB'}, 'LEFT-ARC'), ({'stack_top_id': 1, 'buffer_first_id': 7, 'stack_top_word': 'al', 'buffer_first_word': 'killed', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'VERB'}, 'RIGHT-ARC'), ({'stack_top_id': 1, 'buffer_first_id': 8, 'stack_top_word': 'al', 'buffer_first_word': 'shaikh', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PROPN'}, 'SHIFT'), ({'stack_top_id': 8, 'buffer_first_id': 9, 'stack_top_word': 'shaikh', 'buffer_first_word': 'abdullah', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PROPN'}, 'RIGHT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 10, 'stack_top_word': 'shaikh', 'buffer_first_word': 'al', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PROPN'}, 'RIGHT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 11, 'stack_top_word': 'shaikh', 'buffer_first_word': '-', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT'), ({'stack_top_id': 11, 'buffer_first_id': 12, 'stack_top_word': '-', 'buffer_first_word': 'ani', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'PROPN'}, 'LEFT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 12, 'stack_top_word': 'shaikh', 'buffer_first_word': 'ani', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PROPN'}, 'RIGHT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 13, 'stack_top_word': 'shaikh', 'buffer_first_word': ',', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT'), ({'stack_top_id': 13, 'buffer_first_id': 14, 'stack_top_word': ',', 'buffer_first_word': 'the', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 14, 'buffer_first_id': 15, 'stack_top_word': 'the', 'buffer_first_word': 'preacher', 'stack_top_pos': 'DET', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 13, 'buffer_first_id': 15, 'stack_top_word': ',', 'buffer_first_word': 'preacher', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 15, 'stack_top_word': 'shaikh', 'buffer_first_word': 'preacher', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'NOUN'}, 'RIGHT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 16, 'stack_top_word': 'shaikh', 'buffer_first_word': 'at', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'ADP'}, 'SHIFT'), ({'stack_top_id': 16, 'buffer_first_id': 17, 'stack_top_word': 'at', 'buffer_first_word': 'the', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 17, 'buffer_first_id': 18, 'stack_top_word': 'the', 'buffer_first_word': 'mosque', 'stack_top_pos': 'DET', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 16, 'buffer_first_id': 18, 'stack_top_word': 'at', 'buffer_first_word': 'mosque', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 8, 'buffer_first_id': 18, 'stack_top_word': 'shaikh', 'buffer_first_word': 'mosque', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'NOUN'}, 'SHIFT'), ({'stack_top_id': 18, 'buffer_first_id': 19, 'stack_top_word': 'mosque', 'buffer_first_word': 'in', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'ADP'}, 'SHIFT'), ({'stack_top_id': 19, 'buffer_first_id': 20, 'stack_top_word': 'in', 'buffer_first_word': 'the', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 20, 'buffer_first_id': 21, 'stack_top_word': 'the', 'buffer_first_word': 'town', 'stack_top_pos': 'DET', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 19, 'buffer_first_id': 21, 'stack_top_word': 'in', 'buffer_first_word': 'town', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 18, 'buffer_first_id': 21, 'stack_top_word': 'mosque', 'buffer_first_word': 'town', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'NOUN'}, 'RIGHT-ARC'), ({'stack_top_id': 18, 'buffer_first_id': 22, 'stack_top_word': 'mosque', 'buffer_first_word': 'of', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'ADP'}, 'SHIFT'), ({'stack_top_id': 22, 'buffer_first_id': 23, 'stack_top_word': 'of', 'buffer_first_word': 'qaim', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'PROPN'}, 'LEFT-ARC'), ({'stack_top_id': 18, 'buffer_first_id': 23, 'stack_top_word': 'mosque', 'buffer_first_word': 'qaim', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'PROPN'}, 'SHIFT'), ({'stack_top_id': 23, 'buffer_first_id': 24, 'stack_top_word': 'qaim', 'buffer_first_word': ',', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT'), ({'stack_top_id': 24, 'buffer_first_id': 25, 'stack_top_word': ',', 'buffer_first_word': 'near', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'ADP'}, 'SHIFT'), ({'stack_top_id': 25, 'buffer_first_id': 26, 'stack_top_word': 'near', 'buffer_first_word': 'the', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 26, 'buffer_first_id': 27, 'stack_top_word': 'the', 'buffer_first_word': 'syrian', 'stack_top_pos': 'DET', 'buffer_first_pos': 'ADJ'}, 'SHIFT'), ({'stack_top_id': 27, 'buffer_first_id': 28, 'stack_top_word': 'syrian', 'buffer_first_word': 'border', 'stack_top_pos': 'ADJ', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 26, 'buffer_first_id': 28, 'stack_top_word': 'the', 'buffer_first_word': 'border', 'stack_top_pos': 'DET', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 25, 'buffer_first_id': 28, 'stack_top_word': 'near', 'buffer_first_word': 'border', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 24, 'buffer_first_id': 28, 'stack_top_word': ',', 'buffer_first_word': 'border', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 23, 'buffer_first_id': 28, 'stack_top_word': 'qaim', 'buffer_first_word': 'border', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'NOUN'}, 'SHIFT'), ({'stack_top_id': 28, 'buffer_first_id': 29, 'stack_top_word': 'border', 'buffer_first_word': '.', 'stack_top_pos': 'NOUN', 'buffer_first_pos': 'PUNCT'}, 'SHIFT')]]\n",
            "[[({'stack_top_id': 0, 'buffer_first_id': 1, 'stack_top_word': ':', 'buffer_first_word': 'from', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'ADP'}, 'SHIFT'), ({'stack_top_id': 1, 'buffer_first_id': 2, 'stack_top_word': 'from', 'buffer_first_word': 'the', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 2, 'buffer_first_id': 3, 'stack_top_word': 'the', 'buffer_first_word': 'ap', 'stack_top_pos': 'DET', 'buffer_first_pos': 'PROPN'}, 'LEFT-ARC'), ({'stack_top_id': 1, 'buffer_first_id': 3, 'stack_top_word': 'from', 'buffer_first_word': 'ap', 'stack_top_pos': 'ADP', 'buffer_first_pos': 'PROPN'}, 'LEFT-ARC'), ({'stack_top_id': 0, 'buffer_first_id': 3, 'stack_top_word': ':', 'buffer_first_word': 'ap', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'PROPN'}, 'SHIFT'), ({'stack_top_id': 3, 'buffer_first_id': 4, 'stack_top_word': 'ap', 'buffer_first_word': 'comes', 'stack_top_pos': 'PROPN', 'buffer_first_pos': 'VERB'}, 'LEFT-ARC'), ({'stack_top_id': 0, 'buffer_first_id': 4, 'stack_top_word': ':', 'buffer_first_word': 'comes', 'stack_top_pos': 'PUNCT', 'buffer_first_pos': 'VERB'}, 'SHIFT'), ({'stack_top_id': 4, 'buffer_first_id': 5, 'stack_top_word': 'comes', 'buffer_first_word': 'this', 'stack_top_pos': 'VERB', 'buffer_first_pos': 'DET'}, 'SHIFT'), ({'stack_top_id': 5, 'buffer_first_id': 6, 'stack_top_word': 'this', 'buffer_first_word': 'story', 'stack_top_pos': 'DET', 'buffer_first_pos': 'NOUN'}, 'LEFT-ARC'), ({'stack_top_id': 4, 'buffer_first_id': 6, 'stack_top_word': 'comes', 'buffer_first_word': 'story', 'stack_top_pos': 'VERB', 'buffer_first_pos': 'NOUN'}, 'RIGHT-ARC'), ({'stack_top_id': 4, 'buffer_first_id': 7, 'stack_top_word': 'comes', 'buffer_first_word': ':', 'stack_top_pos': 'VERB', 'buffer_first_pos': 'PUNCT'}, 'RIGHT-ARC')]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import deque\n",
        "from conllu import parse_incr\n",
        "\n",
        "\n",
        "class DepParseDataset:\n",
        "    def __init__(self, file_path):\n",
        "        self.parsed_data = self.load_data(file_path)\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        parsed_entries = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for parsed_sentence in parse_incr(file):\n",
        "                parsing_steps = self._generate_parsing_steps(parsed_sentence)\n",
        "                parsed_entries.append(parsing_steps)\n",
        "        print(parsed_entries[:1])\n",
        "        return parsed_entries\n",
        "\n",
        "    def _generate_parsing_steps(self, parsed_sentence):\n",
        "        parsing_steps = []\n",
        "        parse_stack = [0]\n",
        "        parse_buffer = deque()\n",
        "\n",
        "        for token in parsed_sentence:\n",
        "            token_id = token['id']\n",
        "            if isinstance(token_id, tuple):\n",
        "                if token['form'] == '-':\n",
        "                    continue\n",
        "                parse_buffer.append(token_id[0])\n",
        "            else:\n",
        "                parse_buffer.append(token_id)\n",
        "\n",
        "        while parse_buffer:\n",
        "            parse_action = self._select_parse_action(parse_stack, parse_buffer, parsed_sentence)\n",
        "            token_features = self._extract_token_features(parse_stack, parse_buffer, parsed_sentence)\n",
        "            parsing_steps.append((token_features, parse_action))\n",
        "\n",
        "            if parse_action == 'RIGHT-ARC':\n",
        "                parse_buffer.popleft()\n",
        "            elif parse_action == 'LEFT-ARC':\n",
        "                parse_stack.pop()\n",
        "            elif parse_action == 'SHIFT':\n",
        "                parse_stack.append(parse_buffer.popleft())\n",
        "\n",
        "        return parsing_steps\n",
        "\n",
        "    def _select_parse_action(self, parse_stack, parse_buffer, parsed_sentence):\n",
        "        if len(parse_stack) < 2:\n",
        "            return 'SHIFT'\n",
        "\n",
        "        first_buffer_token = parse_buffer[0] if parse_buffer else None\n",
        "        top_stack_token = parse_stack[-1] if parse_stack else None\n",
        "\n",
        "        if top_stack_token is not None and first_buffer_token is not None:\n",
        "            buffer_head_idx = parsed_sentence[first_buffer_token - 1]['head']\n",
        "            stack_head_idx = parsed_sentence[top_stack_token - 1]['head']\n",
        "\n",
        "            if buffer_head_idx == top_stack_token:\n",
        "                return 'RIGHT-ARC'\n",
        "            elif stack_head_idx == first_buffer_token:\n",
        "                return 'LEFT-ARC'\n",
        "\n",
        "        return 'SHIFT'\n",
        "\n",
        "    def _extract_token_features(self, parse_stack, parse_buffer, parsed_sentence):\n",
        "        features = {\n",
        "            'stack_top_id': 0,\n",
        "            'buffer_first_id': 0,\n",
        "            'stack_top_word': 'NULL',\n",
        "            'buffer_first_word': 'NULL',\n",
        "            'stack_top_pos': 'NULL',\n",
        "            'buffer_first_pos': 'NULL'\n",
        "        }\n",
        "\n",
        "        if parse_stack:\n",
        "            stack_top_token = parsed_sentence[parse_stack[-1] - 1]\n",
        "            features.update({\n",
        "                'stack_top_id': parse_stack[-1],\n",
        "                'stack_top_word': stack_top_token['form'].lower(),\n",
        "                'stack_top_pos': stack_top_token['upos']\n",
        "            })\n",
        "\n",
        "        if parse_buffer:\n",
        "            buffer_first_token = parsed_sentence[parse_buffer[0] - 1]\n",
        "            features.update({\n",
        "                'buffer_first_id': parse_buffer[0],\n",
        "                'buffer_first_word': buffer_first_token['form'].lower(),\n",
        "                'buffer_first_pos': buffer_first_token['upos']\n",
        "            })\n",
        "\n",
        "        return features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.parsed_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.parsed_data[index]\n",
        "\n",
        "\n",
        "train_dataset = DepParseDataset('en_ewt-ud-train.conllu')\n",
        "dev_dataset = DepParseDataset('en_ewt-ud-dev.conllu')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class BERTDependencyParser(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(BERTDependencyParser, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.hidden2action = nn.Linear(768, num_actions)  # BERT base produces 768 features\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0, :]  # Use the representation of [CLS]\n",
        "        action_scores = self.hidden2action(pooled_output)\n",
        "        return torch.log_softmax(action_scores, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pos_vocab(dataset):\n",
        "    pos_tags = set()\n",
        "\n",
        "    # Iterate over each sentence data in the dataset\n",
        "    for sentence_data in dataset:\n",
        "        # Each token is a tuple (features, action)\n",
        "        for features, _ in sentence_data:\n",
        "            # Assuming 'features' is a dictionary containing 'stack_top_pos' and 'buffer_first_pos'\n",
        "            pos_tags.add(features['stack_top_pos'])\n",
        "            pos_tags.add(features['buffer_first_pos'])\n",
        "\n",
        "    # Map each POS tag to a unique index\n",
        "    pos_to_index = {pos: idx for idx, pos in enumerate(sorted(pos_tags))}\n",
        "    pos_to_index['PAD'] = len(pos_to_index) # Adding a padding token for POS tags\n",
        "    return pos_to_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
